# -*- coding: utf-8 -*-
"""3_Telco_Model_Predict.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vXMZ2TB8N7-C2LBUDG5Vo0JXGnPLg8tT

# SUP ML 3 - PREDICT

# Librerias
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

pd.set_option('display.max_columns',None)

import warnings
warnings.filterwarnings("ignore")

from google.colab import drive
drive.mount('/content/drive')

"""# Carga modelo

* Cargar el modelo
* Obtener la lista de model features
"""

import pickle

model = pickle.load(open('/content/drive/MyDrive/DSC 0523– Entregable 2 - Borrero, Dottori, He/Modelo/EJERCICIO-ML-Sup/data/traintest.pkl', 'rb'))

"""# Carga PREDICT dataset"""

X_pred = pd.read_csv('/content/drive/MyDrive/DSC 0523– Entregable 2 - Borrero, Dottori, He/Modelo/EJERCICIO-ML-Sup/data/telecom_churn_PREDICT.csv')
X_pred.head()

# Check features matched features model
features_predict = list(X_pred.columns) #predict dataset : NEW and RAW data . Need to preprocess again
model_features = list(model.feature_names_in_) #data set traintest

model.feature_names_in_

features_model =  list(model.feature_names_in_)
print('Estas son las variables que tenemos que conseguir en el preprocessing para que el modelo funcione:\n', features_model)

features_predict == model_features

"""# ML Preprocessing

Se deben realizar las mismas transformaciones que se aplicaron en el preprocessing del train:
  * Eliminar mismas variables
  * Imputar mismos valores a nulos
  * Aplicar mismo encoding de categoricos
  * ...todo lo necesario para replicar las variables con las que el modelo se entrenó

NO SE DEBEN repetir:
  * Analisis visual de los datos
  * Analisis de correlaciones
  * Analisis de varianza
  * Calculo de metricas que varien con la distribución
"""

X_pred.set_index('Customer_ID', inplace=True)

print('Se define {Customer_ID} como indice')

X_pred.head()

X_pred.info()

X_pred.shape

X_pred.isnull().values.any()

X_pred.info()

X_pred.isnull().sum()

X_pred['rev'].isnull().sum()

X_pred['rev'].max()

X_pred['rev'].min()

X_pred['rev'] = np.where(X_pred['rev']<0, X_pred['rev'] == 0, X_pred['rev'])

X_pred['rev'].min()

X_pred['rev'].fillna(-999, inplace = True)

X_pred['rev'].isnull().sum()

X_pred['mou'].isnull().sum()

X_pred['mou'].max()

X_pred['mou'].min()

X_pred['mou'].fillna(-999, inplace = True)

X_pred['mou'].isnull().sum()

X_pred['totmrc'].max()

X_pred['totmrc'].min()

X_pred['totmrc'] = np.where(X_pred['totmrc']<0, X_pred['totmrc'] == 0, X_pred['totmrc'])

X_pred['totmrc'].isnull().sum()

X_pred['totmrc'].fillna(-999, inplace = True)

X_pred['totmrc'].isnull().sum()

X_pred['da'].isnull().sum()

X_pred['da'].max()

X_pred['da'].min()

X_pred['da'].fillna(-999, inplace = True)

X_pred['da'].isnull().sum()

X_pred['ovrmou'].isnull().sum()

X_pred['ovrmou'].max()

X_pred['ovrmou'].min()

X_pred['ovrmou'].fillna(-999, inplace = True)

X_pred['ovrmou'].isnull().sum()

X_pred['ovrrev'].max()

X_pred['ovrrev'].min()

X_pred['ovrrev'].isnull().sum()

X_pred['ovrrev'].fillna(-999, inplace = True)

X_pred['ovrrev'].isnull().sum()

X_pred['vceovr'].isnull().sum()

X_pred['vceovr'].max()

X_pred['vceovr'].min()

X_pred['vceovr'].fillna(-999, inplace = True)

X_pred['vceovr'].isnull().sum()

X_pred['datovr'].max()

X_pred['datovr'].min()

X_pred['datovr'].isnull().sum()

X_pred['datovr'].fillna(-999, inplace = True)

X_pred['datovr'].isnull().sum()

X_pred['roam'].max()

X_pred['roam'].min()

X_pred['roam'].isnull().sum()

X_pred['roam'].fillna(-999, inplace = True)

X_pred['roam'].isnull().sum()

X_pred['change_mou'].max()

X_pred['change_mou'].min()

X_pred['change_mou'].isnull().sum()

X_pred['change_mou'].fillna(-999, inplace = True)

X_pred['change_mou'].isnull().sum()

X_pred['change_rev'].isnull().sum()

X_pred['change_rev'].max()

X_pred['change_rev'].min()

X_pred['change_rev'].fillna(-999, inplace = True)

X_pred['change_rev'].isnull().sum()

X_pred['drop_vce'].min()

X_pred['drop_vce'].max()

X_pred['drop_vce'].isnull().sum()

X_pred['drop_dat'].max()

X_pred['drop_dat'].min()

X_pred['blck_vce'].min()

X_pred['blck_vce'].max()

X_pred['blck_dat'].max()

X_pred['blck_dat'].min()

X_pred['unan_vce'].max()

X_pred['unan_vce'].min()

X_pred['unan_dat'].max()

X_pred['unan_dat'].min()

X_pred['plcd_vce'].max()

X_pred['plcd_vce'].min()

X_pred['plcd_dat'].max()

X_pred['plcd_dat'].min()

X_pred['recv_vce'].max()

X_pred['recv_vce'].min()

cols_a_revisar = ['recv_sms', 'comp_vce', 'comp_dat',
                  'custcare', 'ccrndmou', 'cc_mou', 'inonemin', 'threeway',
                  'threeway', 'mou_cvce', 'mou_cdat', 'mou_rvce', 'owylis_vce',
                  'mouowylisv']

for i in cols_a_revisar:
  print(X_pred.hist(i, bins = 5))

X_pred['new_cell'].value_counts()

X_pred.isnull().sum()

X_pred.hist('avg6mou')

X_pred['avg6mou'].isnull().sum()

X_pred['avg6mou'].fillna(-999, inplace=True)

X_pred['avg6mou'].isnull().sum()

X_pred.hist('avg6qty')

X_pred['avg6qty'].isnull().sum()

X_pred['avg6qty'].fillna(-999, inplace=True)

X_pred['avg6rev'].isnull().sum()

X_pred.hist('avg6rev')

X_pred['avg6rev'].isnull().sum()

X_pred['avg6rev'].fillna(-999, inplace=True)

X_pred['avg6rev'].isnull().sum()

X_pred['prizm_social_one'].value_counts()

X_pred['prizm_social_one'].isnull().sum()

X_pred['prizm_social_one'].fillna('S', inplace=True)

X_pred['prizm_social_one'].value_counts()

X_pred['prizm_social_one'].isnull().sum()

X_pred['area'].isnull().sum()

X_pred['area'].value_counts()

X_pred['area'].fillna('NEW YORK CITY AREA', inplace=True)

X_pred['area'].value_counts()

X_pred['area'].isnull().sum()

city_areas = {
    'NEW YORK CITY AREA': 'Northeast',
    'DC/MARYLAND/VIRGINIA AREA': 'Mid-Atlantic',
    'MIDWEST AREA': 'Midwest',
    'ATLANTIC SOUTH AREA': 'Southeast',
    'CALIFORNIA NORTH AREA': 'West',
    'DALLAS AREA': 'Southwest',
    'NEW ENGLAND AREA': 'Northeast',
    'SOUTHWEST AREA': 'Southwest',
    'CHICAGO AREA': 'Midwest',
    'LOS ANGELES AREA': 'West',
    'GREAT LAKES AREA': 'Midwest',
    'OHIO AREA': 'Midwest',
    'NORTHWEST/ROCKY MOUNTAIN AREA': 'West',
    'NORTH FLORIDA AREA': 'Southeast',
    'CENTRAL/SOUTH TEXAS AREA': 'Southwest',
    'HOUSTON AREA': 'Southwest',
    'SOUTH FLORIDA AREA': 'Southeast',
    'TENNESSEE AREA': 'Southeast',
    'PHILADELPHIA AREA': 'Mid-Atlantic'
}

X_pred['area'] = X_pred['area'].map(city_areas)

X_pred['area'].value_counts()

X_pred['dualband'].value_counts()

X_pred['dualband'].isnull().sum()

X_pred['dualband'].fillna('Y', inplace=True)

X_pred['dualband'].isnull().sum()

X_pred['refurb_new'].value_counts()

X_pred['refurb_new'].isnull().sum()

X_pred['refurb_new'].fillna('N', inplace=True)

X_pred['refurb_new'].isnull().sum()

X_pred['hnd_price'].isnull().sum()

X_pred.hist('hnd_price')

X_pred['hnd_price'].fillna(-999, inplace=True)

X_pred['hnd_price'].isnull().sum()

X_pred['phones'].isnull().sum()

X_pred['phones'].value_counts()

X_pred['phones'].fillna(1.0, inplace=True)

X_pred['models'].describe()

X_pred.hist('models')

X_pred['models'].isnull().sum()

X_pred['models'].fillna(1.0, inplace=True)

X_pred['models'].isnull().sum()

X_pred['hnd_webcap'].value_counts()

X_pred['hnd_webcap'].isnull().sum()

X_pred['hnd_webcap'].fillna('WCMB', inplace=True)

X_pred['hnd_webcap'].value_counts()

X_pred['hnd_webcap'].isnull().sum()

X_pred['truck'].value_counts()

X_pred['truck'].isnull().sum()

X_pred['truck'].fillna(0.0, inplace=True)

X_pred['truck'].value_counts()

X_pred['truck'].isnull().sum()

X_pred['rv'].value_counts()

X_pred['rv'].fillna(0.0, inplace=True)

X_pred['rv'].value_counts()

X_pred['ownrent'].value_counts()

X_pred['ownrent'].isnull().sum() *100/len(X_pred['ownrent'])

X_pred['ownrent'].value_counts(normalize=True)

X_pred['ownrent'].isnull().sum()

X_pred['ownrent'].fillna('O', inplace=True)

X_pred['ownrent'].isnull().sum()

X_pred.hist('lor')

X_pred['lor'].isnull().sum() *100/len(X_pred['lor'])

X_pred['lor'].fillna(-999, inplace=True)

X_pred['dwlltype'].value_counts()

X_pred['dwlltype'].isnull().sum()

X_pred['dwlltype'].isnull().sum() * 100/len(X_pred['dwlltype'])

X_pred['dwlltype'].value_counts(normalize=True)

X_pred_dwlltype_counts = X_pred["dwlltype"].value_counts()

# Calcular la proporción de cada color en relación con el total de colores no nulos
total_non_null = X_pred_dwlltype_counts.sum()
proportions = X_pred_dwlltype_counts / total_non_null

# Calcular el número de nulos que se asignará a cada color
total_nulls = X_pred["dwlltype"].isnull().sum()

# Manejar el caso cuando no hay valores nulos
if total_nulls == 0:
    print("No hay valores nulos en la columna 'color'.")
else:
    nulls_per_dwlltype = np.round(proportions * total_nulls).astype(int)

    # Distribuir los nulos proporcionalmente en los colores existentes
    null_indices = X_pred[X_pred["dwlltype"].isnull()].index
    dwlltype = nulls_per_dwlltype.index.to_list()
    weights = nulls_per_dwlltype.values / nulls_per_dwlltype.sum()
    replacement_dwlltype = np.random.choice(dwlltype, size=len(null_indices), p=weights)
    X_pred.loc[null_indices, "dwlltype"] = replacement_dwlltype

X_pred['dwlltype'].value_counts()

X_pred['marital'].value_counts()

X_pred['marital'].isnull().sum()

X_pred['marital'].value_counts()

X_pred['marital'].fillna('U', inplace=True)

X_pred['marital'].isnull().sum()

X_pred['adults'].value_counts()

X_pred['adults'].isnull().sum()

# Assuming you have already loaded the DataFrame "df" containing the "adults" column

# Calculate the counts of each category in the "adults" column
X_pred_adults_counts = X_pred["adults"].value_counts()

# Calculate the proportion of each category in relation to the total non-null categories
total_non_null = X_pred_adults_counts.sum()
proportions = X_pred_adults_counts / total_non_null

# Calculate the number of missing values that will be assigned to each category
total_nulls = X_pred["adults"].isnull().sum()

# Handle the case when there are no missing values
if total_nulls == 0:
    print("No hay valores nulos en la columna 'adults'.")
else:
    nulls_per_adults = np.round(proportions * total_nulls).astype(int)

    # Distribute the missing values proportionally among the existing categories
    null_indices = X_pred[X_pred["adults"].isnull()].index
    adults = nulls_per_adults.index.to_list()
    weights = nulls_per_adults.values / nulls_per_adults.sum()
    replacement_adults = np.random.choice(adults, size=len(null_indices), p=weights)
    X_pred.loc[null_indices, "adults"] = replacement_adults

X_pred['adults'].isnull().sum()

X_pred['adults'].value_counts()

X_pred['infobase'].value_counts()

X_pred['infobase'].isnull().sum()*100/len(X_pred['infobase'])

X_pred['infobase'].fillna('M', inplace=True)

X_pred['infobase'].isnull().sum()

X_pred['income'].isnull().sum()

X_pred.hist('income')

X_pred['income'].describe()

X_pred['income'].fillna(-999, inplace=True)

X_pred['income'].isnull().sum()

X_pred['numbcars'].value_counts()

X_pred['numbcars'].fillna(-999.00, inplace=True)

X_pred['numbcars'].isnull().sum()

X_pred['HHstatin'].value_counts()

X_pred['HHstatin'].isnull().sum() * 100/len(X_pred['HHstatin'])

# Assuming you have already loaded the DataFrame "df" containing the "HHstatin" column

# Calculate the counts of each category in the "HHstatin" column
X_pred_HHstatin_counts = X_pred["HHstatin"].value_counts()

# Calculate the proportion of each category in relation to the total non-null categories
total_non_null = X_pred_HHstatin_counts.sum()
proportions = X_pred_HHstatin_counts / total_non_null

# Calculate the number of missing values that will be assigned to each category
total_nulls = X_pred["HHstatin"].isnull().sum()

# Handle the case when there are no missing values
if total_nulls == 0:
    print("No hay valores nulos en la columna 'HHstatin'.")
else:
    nulls_per_category = np.round(proportions * total_nulls).astype(int)

    # Distribute the missing values proportionally among the existing categories
    null_indices = X_pred[X_pred["HHstatin"].isnull()].index
    categories = nulls_per_category.index.to_list()
    weights = nulls_per_category.values / nulls_per_category.sum()
    replacement_HHstatin = np.random.choice(categories, size=len(null_indices), p=weights)
    X_pred.loc[null_indices, "HHstatin"] = replacement_HHstatin

X_pred['HHstatin'].isnull().sum()

X_pred['dwllsize'].value_counts()

X_pred['dwllsize'].isnull().sum()

X_pred['dwllsize'].isnull().sum() * 100/len(X_pred['dwllsize'])

X_pred['dwllsize'].value_counts(normalize=True)

X_pred['dwllsize'].fillna('A', inplace=True)

X_pred['forgntvl'].isnull().sum()

X_pred['forgntvl'].value_counts()

X_pred['forgntvl'].fillna(0.0, inplace=True)

X_pred['forgntvl'].value_counts()

X_pred['ethnic'].value_counts()

X_pred['ethnic'].isnull().sum()

X_pred['ethnic'].fillna('N', inplace=True)

X_pred['ethnic'].isnull().sum()

X_pred['kid0_2'].value_counts()

X_pred['kid0_2'].isnull().sum()

X_pred['kid0_2'].fillna('U', inplace=True)

X_pred['kid0_2'].isnull().sum()

X_pred['kid3_5'].value_counts()

X_pred['kid3_5'].isnull().sum()

X_pred['kid3_5'].fillna('U', inplace=True)

X_pred['kid3_5'].isnull().sum()

X_pred['kid6_10'].value_counts()

X_pred['kid6_10'].isnull().sum()

X_pred['kid6_10'].fillna('U', inplace=True)

X_pred['kid6_10'].isnull().sum()

X_pred['kid11_15'].value_counts()

X_pred['kid11_15'].isnull().sum()

X_pred['kid11_15'].fillna('U', inplace=True)

X_pred['kid11_15'].isnull().sum()

X_pred['kid16_17'].value_counts()

X_pred['kid16_17'].isnull().sum()

X_pred['kid16_17'].fillna('U', inplace=True)

X_pred['kid16_17'].isnull().sum()

X_pred['creditcd'].value_counts()

X_pred['creditcd'].isnull().sum()

X_pred['creditcd'].fillna('Y', inplace=True)

X_pred.hist('eqpdays')

X_pred['eqpdays'].isnull().sum()

X_pred['eqpdays'].fillna(-999, inplace=True)

X_pred['eqpdays'].isnull().sum()

len(X_pred.index.unique())==len(X_pred.index)

X_pred.shape

X_pred.info()

X_pred.shape

if X_pred.isnull().sum().any() == False:

  print('El dataset no contiene nulos a imputar')

del(X_pred["infobase"])

X_pred.head()

target= ["churn"]
def obtener_lista_variables(dataset):
    lista_numericas=[]
    lista_boolean=[]
    lista_categoricas=[]
    for i in dataset:
        if    (dataset[i].dtype.kind=="f" or dataset[i].dtype.kind=="i") and len(dataset[i].unique())!= 2  and i not in target:
              lista_numericas.append(i)
        elif  (dataset[i].dtype.kind=="f" or dataset[i].dtype.kind=="i" or dataset[i].dtype.kind=="b")  and len(dataset[i].unique())== 2  and i not in target:
              lista_boolean.append(i)
        elif  (dataset[i].dtype.kind=="O")  and i not in target:
              lista_categoricas.append(i)

    return lista_numericas, lista_boolean, lista_categoricas

l_num,l_bool,l_cat=obtener_lista_variables(X_pred)

for i in l_bool:
  if i in X_pred:
    X_pred[i+"_bool"]=X_pred[i].astype(int)

for column in l_bool:
    if column in X_pred:
        X_pred = X_pred.drop(column, axis=1)

l_num_2,l_bool_2,l_cat_2 =obtener_lista_variables(X_pred)

l_num_2

l_bool_2

l_cat_2

X_pred = pd.get_dummies(data=X_pred, columns=l_cat_2)

target = 'churn'
features = X_pred.columns[X_pred.columns!=target]

X = X_pred[features]

y = X_pred.columns[X_pred.columns ==target]

def highly_correlated(df, threshold):
    col_corr = list() # Set of all the names of deleted columns
    colnames = list()
    rownames = list()
    corr_matrix = df.corr().abs()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if (corr_matrix.iloc[i, j] >= threshold) and (corr_matrix.columns[j] not in col_corr):
                colnames.append(corr_matrix.columns[i]) # getting the name of column
                rownames.append(corr_matrix.index[j])
                col_corr.append(corr_matrix.iloc[i, j])
    Z = pd.DataFrame({'F1':colnames,
                      'F2':rownames,
                      'corr_F1_F2':col_corr,
                      'corr_F1_target': [np.abs(np.corrcoef(df[i])) for i in colnames],
                      'corr_F2_target': [np.abs(np.corrcoef(df[i])) for i in rownames]
                      })
    Z['F_to_delete'] = rownames
    Z['F_to_delete'][Z['corr_F1_target'] < Z['corr_F2_target']] = Z['F1'][Z['corr_F1_target'] < Z['corr_F2_target']]

    return Z

highly_corr = highly_correlated(X_pred,0.95)
highly_corr

drop_cols = list(highly_corr['F_to_delete'])

X_pred.drop(columns=drop_cols, inplace=True)
print('Eliminadas columnas altamente correlacionadas:', drop_cols)

X_pred.shape

target = 'churn'
features = X_pred.columns[X_pred.columns!=target]

X2 = X_pred[features]

y2 = X_pred.columns[X_pred.columns ==target]

pip install scikit-learn

from sklearn.feature_selection import VarianceThreshold

X2.shape

vt = VarianceThreshold(threshold = 0.01)

vt.fit(X2)

cols_lowvar = X2.columns[vt.get_support()==False]

X2.drop(columns=cols_lowvar,inplace=True)
print(len(cols_lowvar),' low variance features were removed:\n', cols_lowvar.to_list())

X2.shape

"""# Check model features

* Comprobar que tenemos en el dataset preprocesado todas las model features, de lo contrario no podremos hacer predict.
* Ordenar las variables en mismo orden que las model features
"""

features_test = list(X_pred.columns)

print('Columnas en dataset:',len(features_test))
print('Variables en modelos:',len(features_model))
print('¿Match?:', features_model == features_test)

missing_features = [i for i in features_model if i not in features_test]
print('Variables que faltan en el dataset:\n', missing_features)

for col in missing_features:
  X_pred[col]= 0

drop_features = [i for i in features_test if i not in features_model]
print('Variables que debes eliminar de tu dataset:\n', drop_features)

X_pred.drop(columns = drop_features, inplace=True)

# Reordena variables
X_pred = X_pred[features_model]

features_test = list(X_pred.columns)

print('Columnas en dataset:',len(features_test))
print('Variables en modelos:',len(features_model))
print('¿Match?:', features_model == features_test)

"""# Rescaling

* Si se entrenó el modelo con un dataset estandarizado, estandarizar con mismo scaler.
"""

# No hace falta

"""# PREDICT

* predict() y predict_proba()
"""

predictions = model.predict(X_pred)
predictions

predict_proba = model.predict_proba(X_pred)

X_pred.head()

"""# Guarda predicciones

* Guardar las predicciones en data path. Cada fila debe estar etiquetada con el ID.
"""

submission = pd.DataFrame(predictions, index = X_pred.index)

submission.head()

submission.value_counts(normalize=True) * 100

submission.to_csv('/content/drive/MyDrive/DSC 0523– Entregable 2 - Borrero, Dottori, He/Modelo/EJERCICIO-ML-Sup/data/predictions.csv')